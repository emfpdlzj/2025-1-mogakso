# Coursera - andrew ng
지도기계학습: 회귀 및 분류 - 모듈2. 여러 입력 변수를 사용한 회귀 분석

## 다중 선형 회귀

#### 다양한 기능
기존은 Single feature x로 y를 예측했다.  
이제부턴 Feature x가 많아지게된다.

$x_j = j^{th}$ features  
n= number of features  
$\vec{x}^{(i)}$ : features of ith training example
ex: $\vec{x}^{(2)} = [ 2, 5, 34, 15 ] $  
$x_j^{(i)}$ = value of feature j in ith training example

f(x)=wx+b 로 정의했었으나 이젠
f(x)=w1w1 +w2x2+ ..+wnxn + b 가 될 것이다.
이제 가중치도 w벡터로 나타낼 수 있다. 
x는 row vector로 feztures를 나타낼 수 있다.
$f_{\vec{w}, b}(\vec{x}) = \vec{w} \cdot \vec{x} + b$

#### 벡터화 파트 1

벡터화 알고리즘 -> 1.코드가 짧아짐 2.머신러닝 속도 빨라짐

벡터화가 없을 때 -> w,x의 연산에 대해 코드를 다 작성해주어야함. 코드가 길어짐

벡터화
$f_{\vec{w}, b}(\vec{x}) = \vec{w} \cdot \vec{x} + b$
```
f=np.dot(w,x)+b 
```
로 표기가능해진다. n이 커질수록 더 효율적이게된다. 

#### 벡터화 파트 1

벡터화가 없을 때 -> 한 스텝에 하나의 연산씩 수행함.

벡터화 : np.dot(w,x)에선 parallel하게 연산이 이루어진다. 
much less 한 시간안에 연산이 가능해진다. 
effecient -scale to large dataset 


벡터화가 없을 때 -> learning rate 알파값을 계산 후 하나하나 계산 할 것임.    
벡터화 할 경우 한 번에 모든 값에 변경된 결과를 적용함.


### 다중선형 회귀를 위한 그라데이션 하강
- 이제 Vector notation을 사용   
- b is still a number

\[
= \frac{1}{m} \sum_{i=1}^{m} \left( f_{w,b}(x^{(i)}) - y^{(i)} \right) x^{(i)}
\]
여기서 이제 w,x가 vector로 변화하게된다.

Normal equation 
  - only for linear regression
  - solvef for w,b without iteration 

  Disadventage
  -doesn't generalize to other learning algorithm
  slow when number of features is large(>10,000)

결론: normal equation method는 선형 회귀에만 사용가능.
w,b를 찾는데는 gradient descent가 더 추천됨

## 실제 경사도 하강

### 기능 확장 파트1

Plot 사용시 -> axis scale 고려 
- scatterplot 
- contour plot

rescale을 잘 해줘야 feature에 맞게 예쁜 Plot을 얻을 수 있음

### 기능 확장 파트2

Mean normalization

Z score
standard deviation : σ

$ x_1 = (x_1 -µ_1)/σ_1 $

acceptable한 ranges 의 Feautre x는 ok, no rescaling.  
too large or too small -> rescale 

### 수렴을 위한 기울기 하강 확인
to work correctly gradient descent-> 좋은 alpha를 골라야 한다. 

learning curve : 가로축 iteration, 세로축 J

적절한 알파값이라면, 매 반복마다 코스트가 감소할 것이다. 또한 특정 횟수를 넘기면 거의 감소하지 않는단걸 확인 가능하다. 즉 w,b가 수렴한다. 

iterations needed when varies
하지만 몇 번을 반복해야 수렴한다고 말하긴 어렵다. 

epsilon을 도입하여, J가 epsilon보다 적게 감소한다면, 수렴이라고 정의한다.
->최솟값에 가장 가까운 w,b를 찾기위해서임

### Choosing Learning rate

알파가 너무 작으면 오래걸리고, 너무 크면 수렴하지 않을 수 있음

만약 learning curve가 요동친다면, 코드가 잘못됐거나 알파가 너무 큰 것임.
또한 parabola에서 알파가 너무 크다면 코스트가 계속 증가 할 수 있다.

충분히 작은 알파값이 코스트를 매 반복마다 감소시킬 수 있음.


알파값
- 0.001 -> 0.003
- 0.01 -> 0.03
- 0.1
- 1
계속 3정도씩 곱해가면서 시도하다보면, 1은 너무 크다는걸 발견가능. 

### Feature Engineering

예> Housing 문제에서, frontage와 Depth를 x1,x2로 볼지, area=x1*x2=x3로 Newfeature를 생성할지 결정할 수 있다. 

feature engineering: by transforming of combining original feature함. 효율성을 증대시킬 수 있음.

### Polynomial Regression

cube model $f_{\vec{w}, b}(\vec{x}) = w_1x + w_2x^2+ w_3x^3 + b$

feature scaling 

$f_{\vec{w}, b}(\vec{x}) = w_1x + w_2x^(1/2) + b$





